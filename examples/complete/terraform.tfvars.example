# =============================================================================
# Example terraform.tfvars
# =============================================================================
# Copy this file to terraform.tfvars and fill in your values:
#   cp terraform.tfvars.example terraform.tfvars
# =============================================================================

# -----------------------------------------------------------------------------
# Required: Databricks Configuration
# -----------------------------------------------------------------------------

# Your Databricks account ID (UUID format)
# Find this in the Databricks Account Console URL or account settings
databricks_account_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

# Workspace IDs to attach the NCC to
# Find these in Databricks Account Console -> Workspaces
databricks_workspace_ids = [
  "1234567890123456",
  # "2345678901234567",  # Add more workspaces as needed
]

# -----------------------------------------------------------------------------
# Required: Confluent Cloud Configuration
# -----------------------------------------------------------------------------

# Private Link Service alias from Confluent Cloud
# Find this in: Confluent Cloud Console -> Cluster -> Settings -> Networking -> Private Link
# Format: s-xxxxx.privatelink.confluent.cloud
confluent_private_link_service_alias = "s-xxxxx.privatelink.confluent.cloud"

# Confluent cluster ID
# Find this in: Confluent Cloud Console -> Cluster -> Settings -> General
# Format: pkc-xxxxx (Dedicated) or lkc-xxxxx (Standard)
confluent_cluster_id = "pkc-xxxxx"

# Confluent Cloud region (usually matches Azure region)
confluent_region = "eastus"

# -----------------------------------------------------------------------------
# Azure Configuration
# -----------------------------------------------------------------------------

# Resource group name (will be created)
resource_group_name = "rg-confluent-transit"

# Azure region - MUST match your Databricks workspace region
location = "eastus"

# -----------------------------------------------------------------------------
# Network Configuration (Optional - defaults shown)
# -----------------------------------------------------------------------------

# VNet address space
# vnet_address_space = ["10.200.0.0/16"]

# Subnet address prefixes
# lb_subnet_address_prefix = "10.200.1.0/24"
# pe_subnet_address_prefix = "10.200.2.0/24"

# Load Balancer frontend IP (static)
# Leave empty for dynamic allocation
# lb_frontend_ip = "10.200.1.100"

# -----------------------------------------------------------------------------
# Kafka Configuration (Optional - defaults shown)
# -----------------------------------------------------------------------------

# Kafka ports to expose through the Load Balancer
# kafka_ports = [9092]

# Number of brokers (for DNS records)
# broker_count = 6

# -----------------------------------------------------------------------------
# Optional Features
# -----------------------------------------------------------------------------

# Create Private DNS Zone for classic compute access
# Set to false if only using serverless compute
# enable_dns_zone = true

# Auto-approve Databricks PE connection
# Set to false if you want manual approval
# auto_approve_databricks_pe = true

# Subscription IDs to auto-approve on the Private Link Service
# auto_approve_subscription_ids = []

# -----------------------------------------------------------------------------
# Tags (Optional)
# -----------------------------------------------------------------------------

tags = {
  Environment = "production"
  Team        = "data-engineering"
  CostCenter  = "12345"
  Project     = "kafka-streaming"
}
